{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study\n",
    "\n",
    "## Campbell, CA, USA\n",
    "\n",
    "### Aziz Mamatov\n",
    "\n",
    "#### Data Source\n",
    "\n",
    "I selected Campbell, CA as it is the city where I live. The data was downloaded from https://mapzen.com/ and it had the information for the city already. \n",
    "\n",
    "#### Data conversion and cleaning\n",
    "\n",
    "XML file was large and I first selected the sample and converted it to CSV format. I then manipulated the data samples.\n",
    "\n",
    "My final file sizes are:\n",
    "\n",
    "File   |   Size   |\n",
    "---   |   ---   |\n",
    "Downloaded xml file   |   77.5 MB   \n",
    "Sample (1/100) xml file   |   799 kB   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating sample file\n",
    "Sample file was created using sample.py. The following libraries were used and following name variables for sample and full size OSM files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  \n",
    "import pprint\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'campbell_mapzen.osm'\n",
    "sample = 'campbell_map_sample2.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and parsing OSM document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tags\n",
    "\n",
    "I run the code tags.py  and it revealed to have the following number of tags in full file of map :\n",
    "{'node': 333973, 1: 398199, 2: 234249, 3: 5782, 4: 1, 6: 50835, 8: 333973, 'nd': 398199, 'bounds': 1, 'member': 5781, 'tag': 234249, 'relation': 657, 'way': 50178, 'osm': 1})\n",
    "\n",
    "The sample file have much less tags. \n",
    "defaultdict(<type 'int'>, {'node': 3339, 'nd': 4264, 'bounds': 1, 'member': 99, 'tag': 2441, 'relation': 7, 'way': 502, 'osm': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag pattern information\n",
    "\n",
    "I am checking information using `tags_patterns.py` contained within tags - 'k' values to see the patterns associated with tags. I selected regular expressions taken from Case studies. There are four tag categories:\n",
    "\n",
    "  * 'problemchars': 0, \"problemchars\", for tags with problematic characters\n",
    "  * 'lower': 1249, \"lower\", for tags that contain only lowercase letters and are valid\n",
    "  * 'other': 71, \"other\", for other tags that do not fall into the other three categories.\n",
    "  * 'lower_colon': 1121 \"lower_colon\", for otherwise valid tags with a colon in their names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users\n",
    "I will find out the number of unique users contributed to the map by producing a set of unique user IDs ('uid'). There are 646 users in the full file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646\n"
     ]
    }
   ],
   "source": [
    "def get_user(element):\n",
    "    return element.get('uid')\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(full_filename):\n",
    "        user = get_user(element)\n",
    "        if user is not None:\n",
    "            users.add(user)\n",
    "\n",
    "    return len(users)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print process_map(full_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street name wrangling \n",
    "I was auding the xml file using `audit2.py` for 1) street names and 2) Zip codes and there were issues some of which were corrected.\n",
    "\n",
    "There were peculiar names, but because they were so called borrowed words I left them as they are:\n",
    "\n",
    "* 'Madrid': set(['Corte de Madrid']), \n",
    "* 'Plaza': set(['Portal Plaza']), \n",
    "* 'Sorrento': set(['Via Sorrento']), \n",
    "* 'Barcelona': set(['Calle de Barcelona']), \n",
    "* 'El Paseo': set(['El Paseo de Saratoga;])\n",
    "\n",
    "\n",
    "However, there were legitimate mistakes, like:\n",
    "\n",
    "* 'Ln' = Lane\n",
    "* 'Rd' = Road\n",
    "* 'Boulvevard' or 'Blvd' = Boulevard\n",
    "* 'Ave' = Avenue\n",
    "* 'St' = 'Street'\n",
    "\n",
    "\n",
    "For zip code files - there was one zip code in map, which did not start with '95' but with '94'. 94 code belongs to Sunnyvale and for some reason was included to Campbell map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of  audit run:\n",
    "defaultdict(<type 'set'>, {'Winchester': set(['Winchester']), 'Ln': set(['Branham Ln']), 'Rd': set(['Homestead Rd']), '7.1': set(['Hwy 17 PM 7.1']), 'Hill': set(['Blossom Hill']), 'Circle': set(['Winchester Circle', 'Calabazas Creek Circle', 'Greenwood Circle', 'Joseph Circle', 'Bobolink Circle']), 'Expressway': set(['Almaden Expressway', 'Southwest Expressway']), 'Alameda': set(['The Alameda']), 'Highway': set(['Monterey Highway']), 'Real': set(['El Camino Real']), 'Boulvevard': set(['Los Gatos Boulvevard']), '1': set(['Prospect Rd #1']), 'Flores': set(['Terreno De Flores']), 'Marino': set(['Via San Marino']), '6': set(['Pruneridge Ave #6']), 'Volante': set(['Via Volante']), 'Bascom': set(['S. Bascom']), 'Dr': set(['Samaritan Dr', 'Linwood Dr']), 'Esquela': set(['Camina Esquela']), 'Bellomy': set(['Bellomy']), 'Napoli': set(['Via Napoli']), 'Saratoga': set(['El Paseo de Saratoga']), 'Plaza': set(['Portal Plaza']), 'Barcelona': set(['Calle de Barcelona']), 'St': set(['N 5th St', 'Monroe St']), 'Mall': set(['Franklin Mall']), 'Franklin': set(['Franklin']), 'Palamos': set(['Via Palamos']), 'Seville': set(['Corte de Seville']), 'Presada': set(['Paseo Presada']), 'Loop': set(['Infinite Loop']), '4A': set(['Saratoga Avenue Bldg 4A']), 'Sorrento': set(['Via Sorrento']), 'Portofino': set(['Via Portofino']), 'Julian': set(['West Julian']), 'Walk': set(['Paseo de San Antonio Walk']), 'Terrace': set(['Hobart Terrace']), 'Madrid': set(['Corte de Madrid']), 'Blvd': set(['Stevens Creek Blvd', 'Los Gatos Blvd']), 'Ave': set(['Cherry Ave', 'Greenbriar Ave', 'Blake Ave', 'Foxworthy Ave', 'Meridian Ave', 'Westfield Ave', 'The Alameda Ave', 'N Blaney Ave']), 'Row': set(['Santana Row'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Results of corrected street names and codes (sample):\n",
    "* Winchester => Winchester\n",
    "* Branham Ln => Branham Lane\n",
    "* Homestead Rd => Homestead Road\n",
    "* Hwy 17 PM 7.1 => Hwy 17 PM 7.1\n",
    "* Blossom Hill => Blossom Hill\n",
    "* Almaden Expressway => Almaden Expressway\n",
    "* Southwest Expressway => Southwest Expressway\n",
    "* The Alameda => The Alameda\n",
    "* El Camino Real => El Camino Real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zip codes audit\n",
    "Zip codes were audited using `zip_codes_audit.py` file and no discrepancies were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV file preparation\n",
    "\n",
    "Now I will save my XML file to a CSV using `data_csv2.py`. Udacity provided with a starter code that where I added shape_element function. \n",
    "As a result, the following files were prepared from sample file:\n",
    "\n",
    "File name | Size | \n",
    "--- | --- |\n",
    "nodes.csv | 281KB \n",
    "nodes_tags.csv | 11KB \n",
    "ways.csv |   100 KB  \n",
    "ways_nodes.csv | 30KB \n",
    "ways_tags.csv | 74KB \n",
    "\n",
    "Then, I repeated the process on the full file and the files with following size were created:\n",
    "\n",
    "File name | Size | \n",
    "--- | --- |\n",
    "nodes.csv | 28.1MB \n",
    "nodes_tags.csv | 1.1MB \n",
    "ways.csv |   3MB  \n",
    "ways_nodes.csv | 9.4MB \n",
    "ways_tags.csv | 7MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing sql database \n",
    "I prepared sql database file campbell_map2.db (55.3MB) using Mac shell. \n",
    "https://discussions.udacity.com/t/creating-db-file-from-csv-files-with-non-ascii-unicode-characters/174958/2\n",
    "\n",
    "* nodes: \"id\",\n",
    "  \"lat\",\n",
    "  \"lon\",\n",
    "  \"user\",\n",
    "  \"uid\" ,\n",
    "  \"version\" ,\n",
    "  \"changeset\" ,\n",
    "  \"timestamp\" \n",
    "* nodes_tags: \"id\" ,\n",
    "  \"key\" ,\n",
    "  \"value\" ,\n",
    "  \"type\"   \n",
    "* ways: id\" ,\n",
    "  \"user\" ,\n",
    "  \"uid\" ,\n",
    "  \"version\" ,\n",
    "  \"changeset\",\n",
    "  \"timestamp\" \n",
    "* ways_nodes:\n",
    "  \"id\" ,\n",
    "  \"node_id\" ,\n",
    "  \"position\" \n",
    "* ways_tags: \n",
    "  \"id\" ,\n",
    "  \"key\" ,\n",
    "  \"value\" ,\n",
    "  \"type\" \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of the dataset statistics using sql queries\n",
    "\n",
    "#### Top 15 tags used overall\n",
    "\n",
    "QUERY = \"SELECT Allkeys.key, COUNT(key) FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) Allkeys GROUP BY Allkeys.key ORDER BY COUNT (key) DESC LIMIT 15;\"\n",
    "\n",
    "* building,26724\n",
    "* highway,23688\n",
    "* name,16575\n",
    "* county,11073\n",
    "* name_base,9953\n",
    "* name_type,9543\n",
    "* zip_left,8901\n",
    "* cfcc,8794\n",
    "* zip_right,8623\n",
    "* street,7845\n",
    "* housenumber,7740\n",
    "* postcode,6420\n",
    "* height,5973\n",
    "* oneway,4880\n",
    "* source,4657\n",
    "\n",
    "#### Total unique users\n",
    "\n",
    "QUERY = \"SELECT COUNT(DISTINCT(a.user)) FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) a;\"\n",
    "\n",
    "640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of types in ways_tags column\n",
    "\n",
    "QUERY: SELECT type, COUNT(*) as num\n",
    "* FROM ways_tags\n",
    "* GROUP BY type\n",
    "* ORDER BY num DESC\n",
    "* LIMIT 4;\n",
    "\n",
    "  * regular,104965\n",
    "  * tiger,75151\n",
    "  * addr,15719\n",
    "  * turn,2248\n",
    "\n",
    "Similar story is with nodes_tages file. An absolute majority of tags were marked as 'regular' as they did not have any value assigned to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nodes and ways\n",
    "\n",
    "QUERY =  select count(distinct(id)) from nodes;\n",
    "* 333973\n",
    "\n",
    "QUERY = select count(distinct(id)) from ways;\n",
    "* 50178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top users\n",
    "QUERY = \"SELECT B.user, COUNT(*) as num\n",
    "* FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) B\n",
    "* GROUP BY B.user\n",
    "* ORDER BY num DESC\n",
    "* LIMIT 10;\n",
    "----------------------\n",
    "* dannykath,40066\n",
    "* \"Bike Mapper\",39716\n",
    "* nmixter,39534\n",
    "* karitotp,34245\n",
    "* mk408,20125\n",
    "* \"Minh Nguyen\",17964\n",
    "* samely,15114\n",
    "* doug_sfba,14559\n",
    "* beddy,11635\n",
    "* RichRico,10413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top amenities in nodes_tags and ways_tags table\n",
    "These two tables had to be combined in order to pick all the values for certain amenities\n",
    "QUERY = SELECT value, COUNT(*) as num\n",
    "*  SELECT DISTINCT(a.value), COUNT(*) as num\n",
    "*  FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) a \n",
    "*  WHERE key='amenity'\n",
    "*  GROUP BY a.value\n",
    "*  ORDER BY num DESC\n",
    "*  LIMIT 10;\n",
    "------------------\n",
    " * parking,767\n",
    " * restaurant,398\n",
    " * school,247\n",
    " * place_of_worship,212\n",
    " * fast_food,206 \n",
    " * bench,120\n",
    " * cafe,106\n",
    " * fuel,87\n",
    " * bicycle_parking,84\n",
    "\n",
    "#### Rare cuisines \n",
    "* SELECT DISTINCT(a.value), COUNT(*) as num\n",
    "* FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) a \n",
    "* WHERE key='cuisine'\n",
    "* GROUP BY a.value\n",
    "* ORDER BY num ASC\n",
    "* LIMIT 10;\n",
    "\n",
    " * Coffee,1\n",
    " * Coffee_shop,1\n",
    " * Diner,1\n",
    " * Sandwich,1\n",
    " * american;burger,1\n",
    " * bagels,1\n",
    " * bakery,1\n",
    " * boba,1\n",
    " * boba_&_snacks,1\n",
    " * boba_tea,1\n",
    "\n",
    "As it can be seen some of the cuisines are not classified in the best manner - I would probably classify Diner as American as well as Bagels etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Problems with data_sets\n",
    "The OpenStreetMap data of Cambell is of a good quality but there are some typo errors  as well as classification mistakes. I did not have to clean a lot of data, as most probably such cleaning was already performed in the past. Data contains lots of information about amenities but it is not clear whether it is updated regularly. \n",
    "This leads to the main theory behind the open map project - are there are enough diligent contributors to the map?\n",
    "\n",
    "As we can see from sql query the top 10 contributors are pretty close to one another in contributions and it means that there is a good chance of igniting competition. It can be done via gamification. Gamification can be obtained via publicizing the competition via social networks (FB, twitter, reddit) and actually awarding certain distinctions to top contributors overall, by continent, country, state, city etc. Top contributors would be awarded certain ranks.\n",
    "\n",
    "#### Pros: \n",
    "It will motivate contributors to contribute even more and attact new contributors. \n",
    "#### Cons: \n",
    "Somebody will need to moderate and track the process and it may take significant time. There also may be controversies \n",
    "\n",
    "Also, a good way of increasing engagement with the openmaps is to create a user friendly apps in iOS and Android that would allow contributions. \n",
    "#### Pros: \n",
    "Apps will possibly have higher engagements as they are easier to be worked with at any time contributor feels like updating or correcting the map.\n",
    "#### Cons:\n",
    "Somebody will have to create the apps and maintain them. Also, somebody will have to monitor the increased contributions to the map.\n",
    "\n",
    "\n",
    "#### Some technical issues. \n",
    "* Resulting CSV files had repeating IDs for each tag - it would be a good idea to expand the tables by adding columns and have each UID for each row. \n",
    "* Some of the data in openmaps, like classification of restaurants by cuisine was not always done in the best manner - there must be some general classification of cuisines in order not to have too many of them.\n",
    "* An absolute majority of tags were marked as 'regular' during data wrangling as they did not have any value assigned to them. It would be a good practice to either programmatically assign value to them or prompt the contributors to assign value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "https://discussions.udacity.com/t/no-user-and-uid-in-a-node/170245/7\n",
    "https://www.tutorialspoint.com/sqlite/sqlite_using_joins.htm\n",
    "https://gist.github.com/swwelch/f1144229848b407e0a5d13fcb7fbbd6f\n",
    "http://stackoverflow.com/questions/8331469/python-dictionary-to-csv http://stackoverflow.com/questions/15707056/get-time-of-execution-of-a-block-of-code-in-python-2-7 https://wiki.openstreetmap.org/wiki/Category:Features https://discussions.udacity.com/c/nd002-p3-data-wrangling\n",
    "http://www.sqlabs.com/blog/2010/12/getting-the-size-of-an-sqlite-database/ http://scottboms.com/downloads/documentation/markdown_cheatsheet.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Files\n",
    "\n",
    "* `audit2.py`: audit names of streets of the city\n",
    "* `campbell_map_sample_2.osm`: sample file of osm data file\n",
    "* `data_csv2.py`: build CSV files from OSM and clean data\n",
    "* `P3_OpenStreetMap_CaseStudy2.pdf`: report file\n",
    "* `sample.py`: sample data extraction code\n",
    "* `sql_queries_project`: SQLITE queries \n",
    "* `tags.py`: tags code\n",
    "* `tags_patterns.py`: tags patterns code\n",
    "* `campbell_map2`: SQLITE database file \n",
    "* `nodes_tags.csv`: csv file\n",
    "* `nodes.csv`: csv file\n",
    "* `ways_nodes.csv`: csv file\n",
    "* `ways_tags.csv`: csv file\n",
    "* `zip_codes_audt.py`: audit file for zip codes\n",
    "* `campbell_mapzen.osm`: full OpenMap extraction file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
